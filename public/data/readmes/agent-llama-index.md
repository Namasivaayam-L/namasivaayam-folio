# ðŸ§  HITL-Enhanced LLM Agent with Redis Memory (FastAPI + LlamaIndex)

An enterprise-ready, multi-tool LLM agent framework built using **FastAPI**, **llama-index**, and **Redis**. It supports **human-in-the-loop (HITL)** workflows, **function calling**, and **session persistence**.

---

## ðŸš€ Features

- âœ… LLM Function Calling via `llama-index` with `FunctionToolWithContext`
- âœ… Human Approval for Risky Tools (HITL)
- âœ… Redis-Powered Chat Memory per `user_id + session_id`
- âœ… WebSocket-based Realtime Interaction
- âœ… Phoenix/OTEL observability tracing
- âœ… Modular Tool Registration

---

## ðŸ§° Tech Stack

| Component        | Tech                       |
|------------------|----------------------------|
| Backend          | FastAPI                    |
| LLM Integration  | LlamaIndex + OpenInstruct  |
| Chat Store       | Redis (via `RedisChatStore`) |
| LLMs Used        | LLaMA 3.1 8B (function-calling) |
| Observability    | Phoenix + OTEL             |

---

## ðŸ› ï¸ Setup Instructions

```bash
git clone https://github.com/Namasivaayam-L/agent-llama-index.git
cd agent-llama-index
poetry install
```

### ðŸ”§ Redis
Ensure Redis is running locally:

```bash
sudo service redis-server start
```

---

## ðŸ§‘â€ðŸ’» Run the Server

```bash
python app.py
```

Server will be running at: `http://localhost:5000`

---

## ðŸ”Œ WebSocket Usage

### Endpoint

```text
ws://localhost:5000/chat
```

### Sample JSON Input

```json
{
  "user_id": "user_001",
  "session_id": "session_abc",
  "input": {
    "query": "How do I reset my password?"
  }
}
```

If a tool requires human approval, send:

```json
{
  "user_id": "user_001",
  "session_id": "session_abc",
  "input": {
    "query": "y"
  }
}
```

---

## ðŸ§  Current Tooling Setup

### ðŸŸ¢ No Approval Needed

| Tool                     | Purpose                                 |
|--------------------------|-----------------------------------------|
| `fetch_customer_chat_history` | Pull session history from Redis     |
| `faq_lookup_tool`        | Answer FAQs based on rules or lookup    |
| `ticket_status_checker`  | Check support ticket status             |
| `suggest_next_step`      | Recommend action based on issue intent  |

### ðŸ”’ Requires Approval (HITL)

| Tool                           | Purpose                            |
|--------------------------------|------------------------------------|
| `get_product_doc_snippet`      | Provide help based on error codes  |
| `raise_ticket_on_behalf`       | Draft a support ticket             |
| `compose_response_for_complex_issue` | Draft a human-like support reply |

---

## ðŸ” Session & History APIs

### Get User Sessions
```http
GET /sessions/user/{user_id}
```

### Get Session Messages
```http
GET /sessions/{user_id}/{session_id}
```

---

## ðŸ” System Prompt (LLM Instruction)

> You are an enterprise-grade support agent assistant. You are helpful, calm, and precise. Your primary job is to understand the customer's issue, retrieve relevant data, suggest tools, and decide whether a human agent must be involved.  
>  
> You have access to tools. Use them when necessary. Some tools require human approvalâ€”ask before using them. Store conversations in Redis. Avoid hallucinations. Be neutral, focused, and context-aware.

---

## ðŸ“ˆ Observability

- ðŸ”— Exposed via [Phoenix](https://arize.com/phoenix/)
- Traces streamed to: `http://localhost:6006/v1/traces`

---

## ðŸ“‚ Project Structure

```
.
â”œâ”€â”€ main.py                        # FastAPI + WebSocket entrypoint
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â”œâ”€â”€ func_agent.py         # FuncAgentWorkflow class
â”‚   â”‚   â””â”€â”€ tools.py              # Tool definitions and registration
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ func_tool_with_ctx.py # FunctionToolWithContext
â”‚   â”‚   â”œâ”€â”€ llms.py               # LLM model mapping
â”‚   â”‚   â””â”€â”€ pydantic_models.py    # Request schemas
â”œâ”€â”€ config/
â”‚   â””â”€â”€ logging.py                # Logging setup
```

---

## ðŸ§ª Testing with Python Client

```python
import asyncio, json, websockets

async def test():
    uri = "ws://localhost:5000/chat"
    async with websockets.connect(uri) as ws:
        await ws.send(json.dumps({
            "user_id": "user_001",
            "session_id": "session_abc",
            "input": { "query": "Check status of ticket ID: 54892" }
        }))
        while True:
            response = await ws.recv()
            print("â†’", response)
asyncio.run(test())
```

---

Built by **Mr. Namasivaayam L.**

---